# -*- coding: utf-8 -*-
"""Nokia_GNews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q-HkubxvM4TMH3AaHMLmlokVPSZLOp2g
"""

# !pip install vaderSentiment

#Loading libraries
import pandas as pd
import pandas_datareader as web
import numpy as np
from openpyxl import load_workbook
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from numpy import array
from datetime import timedelta
import openpyxl
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU
import time
import os

#LOAD WORKBOOK TELECOM_INPUT_FILE FOR FETCHING INPUT.
#Define variable to load the dataframe from the file.
READ_FILE = openpyxl.load_workbook("TELECOM_INPUT_FILE.xlsx")
#Define variable to read sheet.
FILE = READ_FILE.active

re = pd.read_excel('TELECOM_INPUT_FILE.xlsx')
re

def clean_url(searched_item,end_time, data_filter):
  x = end_time
  today =str(x)[:10]
  start_time = str(data_filter)[:10]
  time =  'after%3A'+ start_time + '+before%3A' + today
  # yesterday = str(x + pd.Timedelta(days=-1))[:10]
  # this_week = str(x + pd.Timedelta(days=-7))[:10]
  # if data_filter == 'today':
  #   time = 'after%3A' + yesterday
  # elif data_filter == 'this_week':
  #   time = 'after%3A'+ this_week + '+before%3A' + today
  # elif data_filter == 'this_year':
  #   time = 'after%3A'+str(x.year -1)
  # elif str(data_filter).isdigit():
  #   temp_time = str(x + pd.Timedelta(days=-int(data_filter)))[:10]
  #   time =  'after%3A'+ temp_time + '+before%3A' + today
  # else:
  #   time=str(data)
  url = f'https://news.google.com/rss/search?q={search}+'+time+'&hl=en-US&gl=US&ceid=US%3Aen'
  return url
                          
def get_news(search, company,end_time, data_filter=None):                                
  url = clean_url(search,end_time, data_filter)
  response = requests.get(url)
  root= ET.fromstring(response.text)
  title = [i.text for i in root.findall('.//channel/item/title') ]
  pubDate = [i.text for i in root.findall('.//channel/item/pubDate') ]
  source = [i.text for i in root.findall('.//channel/item/source') ]
  df = pd.DataFrame({'DATE':pubDate,'TITLE':title,'SOURCE':source })
  # df.DATE = pd.to_datetime(df.DATE, unit='ns')
  df['Company'] = company
  df.set_index('DATE',inplace=True)#Set index
  df.index = pd.to_datetime(df.index,errors='coerce').tz_localize(None)
  analyzer = SentimentIntensityAnalyzer()
  sentiment = [ ]
  positive = [ ]
  negative = [ ]
  neutral = [ ]


  for n in range(df.shape[0]):
    Tweets = df.iloc[n,0]
    Tweets_analyzed = analyzer.polarity_scores(Tweets)
    sentiment.append(Tweets_analyzed['compound'])
    positive.append(Tweets_analyzed['pos'])
    negative.append(Tweets_analyzed['neg'])
    neutral.append(Tweets_analyzed['neu'])

  df["Sentiment"] = sentiment
  df["Positive"] = positive
  df["Negative"] = negative
  df["Neutral"] = neutral

  # df.to_csv(f'{search}_news.csv', encoding='utf-8-sig')
  return df

#Iterate through the country column of file to print the country name.
i = 1

for col in FILE.iter_cols(6,6):
  start_time = col[1].value
  if isinstance(start_time,datetime):
    start_time = start_time.date()
for col in FILE.iter_cols(7,7):
  end_time = col[1].value
  if isinstance(end_time,datetime):
    end_time = end_time.date()

for col in FILE.iter_cols(2,2):
  COUNTRY = col[i].value
  COUNTRY = print(f'COUNTRY : {COUNTRY}\n')

j = 2
for col in FILE.iter_cols(3,3):
  KPI = col[j].value
  KPI = print(f'KPI : {KPI}\n')

x = 2
while(x<=15):
  for col in FILE.iter_cols(5,5):
    search = col[x].value
    search_term = print(f'SEARCH: {search}\n')
  for col in FILE.iter_cols(4,4):
    company=col[x].value
    COMPANY =print(f'Company NAME: {company}\n')
  x += 3

        # start = time.time()
        # df = get_news(search, company, data_filter= 165)
        # end = time.time()-start  

  if os.path.isfile('News_All.csv')==False:# if file doesnt exist just to_csv this 
    df = get_news(search,company,end_time,start_time)  
    df.sort_index(ascending=False,inplace=True)
    print('making file')
          # print(df)
    df.to_csv('News_All.csv',encoding='utf-8')
  else:# If file exists
    df_og = pd.read_csv('News_All.csv',encoding='utf-8')
    df_og.set_index('DATE',inplace=True)
    df_og.sort_index(inplace=True)
    # print(df_og)
    if df_og[df_og.Company==company].empty:# If that comapny data doesnt exist
      df = get_news(search,company,end_time,start_time)
      print(f'{company} data')
            # print(df)
      df_final=pd.concat([df_og,df])
      format = '%Y-%m-%d %H:%M:%S'
      df_final.index= pd.to_datetime(df_final.index, format=format)
      df_final.sort_index(ascending=False,inplace=True)# Simple concat
      df_final.to_csv('News_All.csv',encoding='utf-8')
            
    else:
      start_index = datetime.strptime(df_og[df_og.Company==company].index[-1],'%Y-%m-%d %H:%M:%S').date()
      start_index = start_index+timedelta(days=1)
            # print(start_index)
            # print(end_time)
      df=get_news(search,company,end_time,start_index)
      print('date change')
            # print(df)
      df_final = pd.concat([df_og,df])
      format = '%Y-%m-%d %H:%M:%S'
      df_final.index= pd.to_datetime(df_final.index, format=format)
      df_final.sort_index(ascending=False,inplace=True)
      df_final.to_csv('News_All.csv',encoding='utf-8') 
        # df.to_csv(f'News.csv')
    print('--------PROCESS COMPLETED--------\n')

FILE['G2'] = (end_time+timedelta(days=1))
READ_FILE.save('TELECOM_INPUT_FILE.xlsx')

def resample_df(df):
  format = '%Y-%m-%d %H:%M:%S'
  le = df.drop(['TITLE','SOURCE'],axis=1)
  le.DATE = pd.to_datetime(le.DATE,format=format)
  re =  le.set_index(['DATE']).groupby(pd.Grouper('Company')).resample('D').mean().dropna(axis=0).reset_index()
  re = re.set_index('DATE').sort_index()
  re.to_csv('News.csv')
  return re
ml=pd.read_csv('News_All')
mn=resample_df(ml)
mn

mn.Company.unique()

import matplotlib.pyplot as plt
def loss_plot(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(len(loss))

  plt.plot(epochs, loss, 'r', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend(loc=0)
  plt.figure()
  plt.show()

def lstm_model(x_train,y_train,shape):
  model = Sequential()
  model.add(LSTM(128, return_sequences=True, input_shape=(shape, 1)))
  model.add(Dropout(0.4))
  model.add(LSTM(128, return_sequences=False))
  # model.add(Dropout(0.4))
  model.add(Dense(64))
  model.add(Dropout(0.2))
  model.add(Dense(1))

                        #Compile the model
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.008), loss='mean_squared_error')
  reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.001)
                        #Train the model
  # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,mode="min")
  history = model.fit(x_train, y_train, batch_size=1, epochs=100,validation_split=0.1,verbose=1,callbacks=[reduce_lr])
  loss_plot(history)
  return model

def split_training_data(train_data,history_range):
  #Split the data into x_train and y_train data sets
  x_train = []
  y_train = []
  for i in range(len(train_data)):
    end_ix = i + history_range
    if end_ix >= len(train_data):
      break
        # gather input and output parts of the pattern
    seq_x, seq_y = train_data[i:end_ix], train_data[end_ix]
    x_train.append(seq_x)
    y_train.append(seq_y)
        
  X = np.array(x_train)
  y = np.array(y_train)
  # print(X)
  X = np.reshape(X, (X.shape[0], X.shape[1], 1))
  return X, y  

def plot_results(historical_data,preds,legend,search_term):
  plt.figure(figsize=(16,8))
  plt.title('Sentiment Forecast of  '+f'{search_term}')
  plt.xlabel('Date', fontsize=18)
  plt.ylabel('Sentiment', fontsize=18)
  plt.plot(historical_data)
  plt.plot(preds)
  plt.legend(legend, loc='lower right')
  plt.show()

import math
from sklearn.preprocessing import MinMaxScaler
def model_dev_viz(search_term,company_name):
  df = pd.read_csv('News.csv')
  df = df[df.Company==company_name]
  if len(df)==0:
    print('No data for this time period maybe due to weekend')
    return 
  # Stock prediction using LSTM
  print(f"DF Shape - {df.shape}")
  data = df.set_index('DATE')
  data.index=pd.to_datetime(data.index)
  # Convert the dataframe to a numpy array
  dataset = data['Sentiment'].values
  # print(dataset)
  # print(dataset)
  # Get the number of rows to train the model on
  training_data_len = math.ceil( len(dataset) * 0.8 )
  # Create the training data set 
  train_data = dataset[:training_data_len]
  # print(train_data)
  history_range = 5
  # Convert the x_train and y_train to numpy arrays
  x_train, y_train = split_training_data(train_data,history_range)

  # Reshape the data
  # x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
  print(f"Train X shape -  {x_train.shape}")

  # Build the LSTM model
  model = lstm_model(x_train,y_train,x_train.shape[1])
  # GRU Model
  # model = opt_gru_model(x_train,y_train,x_train.shape[1])
  # print(model.summary())
  # Create the testing data set
  # test_model(scaled_data,dataset,training_data_len)
  test_data = dataset[training_data_len - history_range:]
  # print('Test Data - ')
  # print(test_data)
  # Create the data set x_test and y_test
  x_test = []
  y_test = dataset[training_data_len:]
  for i in range(history_range, len(test_data)):
    x_test.append(test_data[i-history_range:i])
  # Convert the data to a numpy array
  x_test = np.array(x_test)

  # Reshape the data
  x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

  # Get the model's predicted price values for the x_test data set
  predictions = model.predict(x_test)
  # predictions = scaler.inverse_transform(predictions)
  # print(f'Predictions - \n {predictions}')

  # Evaluate model (get the root mean quared error (RMSE))
  rmse = np.sqrt( np.mean( predictions - y_test )**2 )
  print(f"rmse error on test dataset- {rmse}")
  
  train = data[:training_data_len]
  valid = data[training_data_len:]
  # print(train)
  valid['Predictions'] = predictions
  # Visualize the data
  plot_results(train['Sentiment'],valid[['Sentiment', 'Predictions']],['Train', 'Val', 'Predictions'],search_term)
  # Future prediction data
  X_FUTURE = 31
  predictions = np.array([])
  last = x_test[-1]
  for i in range(X_FUTURE):
    curr_prediction = model.predict(np.array([last]))
    last = np.concatenate([last[1:], curr_prediction])
    predictions = np.concatenate([predictions, curr_prediction[0]])
  # predictions = scaler.inverse_transform([predictions])[0]

  dicts = []
  # print(data)
  curr_date = data.index[-1]
  print(curr_date)
  for i in range(X_FUTURE):
    curr_date = curr_date + timedelta(days=1)
    dicts.append({'Predicted_Sentiment':predictions[i], "Future_Date": curr_date})

  new_data = pd.DataFrame(dicts)
  new_data['Company'] = company_name
  if os.path.isfile('News1.csv')==False:
    new_data.to_csv('News1.csv',index=False)
  else:
    pr = pd.read_csv('News1.csv')
    new_pr = pd.concat([pr,new_data])
    new_pr.to_csv('News1.csv',index=False)
  # append_predictions(new_data,company_name)
  # Plot the data
  train = data
  # Visualize the data
  new_data.set_index('Future_Date',inplace=True)
  plot_results(train['Sentiment'],new_data["Predicted_Sentiment"],['Train', 'Predictions'],search_term)
  
  print('--------PROCESS COMPLETED--------\n')

x = 2
while(x<=15):
  for col in FILE.iter_cols(5,5):
    search = col[x].value
    search_term = print(f'SEARCH: {search}\n')
  for col in FILE.iter_cols(4,4):
    company=col[x].value
    COMPANY =print(f'Company NAME: {company}\n')
  x += 3

        # start = time.time()
        # df = get_news(search, company, data_filter= 165)
        # end = time.time()-start  
  model_dev_viz(search_term,company)
  print('--------PROCESS COMPLETED--------\n')

re = pd.read_csv('/content/News1.csv')
re