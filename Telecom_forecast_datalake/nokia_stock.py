# -*- coding: utf-8 -*-
"""Copy of Nokia_Stock.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VLt7rA4J8QJ0ElZkGINn9slHz1MaKade
"""

# !pip install pandas==1.5.2
# !pip install pandas_datareader==0.10.0
# !pip install xlsxwriter==3.0.3
# !pip install yfinance
# !pip install keras_tuner==1.1.3

#Loading libraries
#XlsxWriter-3.0.3
import pandas as pd# pandas-1.5.2
import pandas_datareader as web# pandas_datareader-0.10.0
import numpy as np
from openpyxl import load_workbook
import requests
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU
import math
from numpy import array
from datetime import timedelta
from sklearn.metrics import mean_squared_error
import openpyxl
import os
from dateutil.parser import parse
# from keras_tuner.tuners import BayesianOptimization # keras_tuner-1.1.3
from pandas_datareader import data as pdr

import yfinance as yf
yf.pdr_override()

#LOAD WORKBOOK TELECOM_INPUT_FILE FOR FETCHING INPUT.
#Define variable to load the dataframe from the file.
READ_FILE = openpyxl.load_workbook("TELECOM_INPUT_FILE.xlsx")
#Define variable to read sheet.
FILE = READ_FILE.active
#Iterate through the country column of file to print the country name.

"""### To Append to Existing CSV
Easiest solution is to import as dataframes, manipulate here then put back to CSV
"""

def append_to_csv(fpath, df):
  df2 = pd.read_csv(fpath)
  df2.Date = pd.to_datetime(df2.Date, format='%Y-%m-%d',utc=True).dt.date
  df2.set_index('Date',inplace=True)
  df3 = pd.concat([df,df2])
  df3.sort_index(ascending=False,inplace=True)
  df3.to_csv(fpath)
  # print(df3)

"""## Fetching Data
Checks if file exists, if not fetch entire dataset from start date to end. \ 

If it does exist, load the existing file check the most recent date and if end_date is greater than that recent date, use the recent date as the start date. And fetch the datat normally, now the dataframe has data from most recent date till end date. We append this dataframe to the existing file. \
In the end the dataframe containing entire data is returned.

"""

def fetch_data(search_term:str,company_name:str,start_date,end_date):
  if (os.path.isfile('Stock_Data.csv'))==False:
    df = pdr.get_data_yahoo([search_term], start=start_date, end=end_date)#To fetch data from 
    df['Company']=company_name
    df['Status']='Actual'
    df.reset_index(inplace=True)
    df.Date = pd.to_datetime(df.Date, format='%Y-%m-%d',utc=True).dt.date
    df.set_index('Date',inplace=True)
    df.sort_index(inplace=True,ascending=False)
    df.to_csv('Stock_Data.csv')# saving data in csv file
  # File Exists, already
  else:
    stock_data = pd.read_csv('Stock_Data.csv')
    stock_data.Date=pd.to_datetime(stock_data.Date, format='%Y-%m-%d',utc=True).dt.date
    stock_data.set_index(keys='Date',inplace=True)
    #Define variable to read sheet.
    # Sort data to format acceptable for lstm
    if stock_data.index.is_monotonic_decreasing:
      stock_data.sort_index(inplace=True)
    # Get the company data from the file
    company_stock_data = stock_data[(stock_data['Company']==company_name) & (stock_data.Status=='Actual')]
    # If this slice is empty, that means we're entering data for first time
    if company_stock_data.empty==False:
      # train_first=True
      # If its not empty, that means we check for latest date
      replacement_start_date = company_stock_data.index[-1]
      if replacement_start_date < end_date:
        start_date = replacement_start_date+timedelta(days=1)
        df = pdr.get_data_yahoo(search_term, start=start_date, end=end_date)#To fetch data from
        if len(df)==0:
          return df
        df.index = df.index.date
    # Append the data to the file, and set company column to the company name
        df['Company']=company_name
        df['Status']='Actual'
        mask2 = ((stock_data['Company']==company_name))
        stock_data.loc[(stock_data.index.isin(df.index)) & mask2] = df
        stock_data.sort_index(ascending=False,inplace=True)
        stock_data.to_csv('Stock_Data.csv')
    # We read data from web, making sure the right start,end dates
    else:
      df = pdr.get_data_yahoo(search_term, start=start_date, end=end_date)#To fetch data from 
    # Append the data to the file, and set company column to the company name
      df['Company']=company_name
      df['Status']='Actual'
      append_to_csv('Stock_Data.csv', df)

# Read the whole file
  df = pd.read_csv('Stock_Data.csv')
  df.Date =pd.to_datetime(df.Date, format='%Y-%m-%d',utc=True).dt.date
  df.set_index('Date',inplace=True)
  df.sort_index(inplace=True)
  # Sort the descending order data to ascending order and return the data with train first.
  return df[(df.Company==company_name) & (df.Status=='Actual')]

"""To plot the loss on training data and validation data"""

def loss_plot(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(len(loss))

  plt.plot(epochs, loss, 'r', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend(loc=0)
  plt.figure()


  plt.show()

"""LSTM Model Architecture"""

def lstm_model(x_train,y_train,shape):
  model = Sequential()
  model.add(LSTM(512, return_sequences=True, input_shape=(shape, 1)))
  model.add(Dropout(0.2))
  model.add(LSTM(256, return_sequences=True))
  model.add(Dropout(0.4))
  model.add(LSTM(128, return_sequences=False))
  model.add(Dropout(0.2))
  model.add(Dense(64))
  # model.add(Dropout(0.2))
  model.add(Dense(1,activation='linear'))

                        #Compile the model
  model.compile(optimizer='adam', loss=tf.keras.losses.Huber())
  print(model.summary())
                        #Train the model
  # early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,mode="min")
  reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)
  history = model.fit(x_train, y_train, batch_size=32, epochs=50,validation_split=0.1,verbose=1,callbacks=[reduce_lr])
  loss_plot(history)
  return model

"""Splitting of training data and converting this time series to supervised learning problem by making x = data of prev 2 months, and y = data of today"""

def split_training_data(train_data,history_range):
  #Split the data into x_train and y_train data sets
  x_train = []
  y_train = []
  n_future = 31   # Number of days we want to look into the future based on the past days.
  n_past = history_range  # Number of past days we want to use to predict the future.

#Reformat input data into a shape: (n_samples x timesteps x n_features)
#In my example, my df_for_training_scaled has a shape (12823, 5)
#12823 refers to the number of data points and 5 refers to the columns (multi-variables).
  for i in range(n_past, len(train_data) - n_future +1):
    x_train.append(train_data[i - n_past:i, :])
    y_train.append(train_data[i:i + 1,:])
  
  X,y = np.array(x_train), np.array(y_train) 
  return X, y

"""### Results are plotted
Result 1: Checking Model performance on existing data \
Result 2: Making Future predictions for next month
"""

def plot_results(historical_data,preds,legend,search_term):
  plt.figure(figsize=(16,8))
  plt.title('Close Price Prediction of  '+f'{search_term}')
  plt.xlabel('Date', fontsize=18)
  plt.ylabel('Close Price', fontsize=18)
  plt.plot(historical_data)
  plt.plot(preds)
  plt.legend(legend, loc='lower right')
  plt.show()

"""Append predictions to appropriate columns"""

def append_predictions(preds,company_name):
  stock_data=pd.read_csv('Stock_Data.csv')
  stock_data.Date = pd.to_datetime(stock_data.Date,format='%Y-%m-%d',utc=True).dt.date
  stock_data.set_index('Date',inplace=True)
  if stock_data.index.is_monotonic_decreasing:
      stock_data.sort_index(inplace=True)
  if stock_data[(stock_data.Company==company_name) & (stock_data['Status']=='Predicted')].empty:
    d = pd.concat([stock_data,preds])
    
  else:
    end = stock_data[(stock_data.Company==company_name) & (stock_data['Status']=='Predicted')].index[-1]
    stock_data.loc[(stock_data.Company==company_name) & (stock_data['Status']=='Predicted')]=preds.loc[preds.index<=end]
    d = pd.concat([stock_data,preds[preds.index>end]])
    
  d.reset_index(inplace=True)
  d.Date = pd.to_datetime(d.Date,format='%Y-%m-%d',utc=True).dt.date
  d.set_index('Date',inplace=True)
  d.sort_index(ascending=False,inplace=True)
  d.to_csv('Stock_Data.csv')

"""Main Model Function"""

def model_dev_viz(search_term,company_name,start_date,end_date):
  df = fetch_data(search_term,company_name,start_date,end_date)
  df.dropna(axis=0,inplace=True,how='any')
  # print(df)
  if len(df)==0:
    print('No data for this time period maybe due to weekend or holiday')
    return 
  # Stock prediction using LSTM
  print(f"DF Shape - {df.shape}")

  data = df.filter(['Close'])
  # Convert the dataframe to a numpy array
  dataset = data.values

  # Get the number of rows to train the model on
  training_data_len = math.ceil( len(dataset) * 0.8 )
  # Scale the data
  scaler = MinMaxScaler(feature_range=(0,1))
  scaled_data = scaler.fit_transform(dataset)
  # Create the training data set 
  # Create the scaled training data set
  train_data = scaled_data[:training_data_len, :]

  history_range = 45
  # Convert the x_train and y_train to numpy arrays
  x_train, y_train = split_training_data(train_data,history_range)

  # Reshape the data
  x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
  print(f"Train X shape -  {x_train.shape}")

  # Build the LSTM model
  model = lstm_model(x_train,y_train,x_train.shape[1])
  # GRU Model
  # model = opt_gru_model(x_train,y_train,x_train.shape[1])
  # print(model.summary())
  # Create the testing data set
  # test_model(scaled_data,dataset,training_data_len)
  test_data = scaled_data[training_data_len - history_range:]

  # Create the data set x_test and y_test
  x_test = []
  y_test = dataset[training_data_len:, :]
  for i in range(history_range, len(test_data)):
    x_test.append(test_data[i-history_range:i, 0])
  # Convert the data to a numpy array
  x_test = np.array(x_test)

  # Reshape the data
  x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

  # Get the model's predicted price values for the x_test data set
  predictions = model.predict(x_test)
  predictions = scaler.inverse_transform(predictions)
  # print(f'Predictions - \n {predictions}')

  # Evaluate model (get the root mean quared error (RMSE))
  rmse = np.sqrt( np.mean( predictions - y_test )**2 )
  print(f"rmse error on test dataset- {rmse}")
  
  train = data[:training_data_len]
  valid = data[training_data_len:]
  valid['Predictions'] = predictions
  # print(predictions)
  # Visualize the data
  plot_results(train['Close'],valid[['Close', 'Predictions']],['Train', 'Val', 'Predictions'],search_term)
  # Future prediction data
  X_FUTURE = 31
  predictions = np.array([])
  last = x_test[-1]
  for i in range(X_FUTURE):
    curr_prediction = model.predict(np.array([last]))
    last = np.concatenate([last[1:], curr_prediction])
    predictions = np.concatenate([predictions, curr_prediction[0]])
  predictions = scaler.inverse_transform([predictions])[0]

  dicts = []
  curr_date = data.index[-1]
  for i in range(X_FUTURE):
    curr_date = curr_date + timedelta(days=1)
    dicts.append({'Close':predictions[i], "Date": curr_date})

  new_data = pd.DataFrame(dicts).set_index("Date")
  new_data['Company'] = company_name
  new_data['Status']='Predicted'
  append_predictions(new_data,company_name)
  # Plot the data
  train = data
  # Visualize the data
  plot_results(train['Close'],new_data['Close'],['Train', 'Predictions'],search_term)
  
  print('--------PROCESS COMPLETED--------\n')

i = 1
for col in FILE.iter_cols(6,6):
  start_date = col[1].value.date()
for col in FILE.iter_cols(7,7):
  end_date = col[1].value
  if isinstance(end_date,datetime):
    end_date = end_date.date()
end_date=end_date+timedelta(days=1)
# Weekends: we dont get stock data
for col in FILE.iter_cols(2,2):
  # Iterate over country column
      COUNTRY = col[i].value
      COUNTRY = print(f'COUNTRY : {COUNTRY}\n')
      j = 3
for col in FILE.iter_cols(3,3):# Iterate over KPI column
  KPI = col[j].value
  KPI = print(f'KPI : {KPI}\n')

y = 3
while(y<=15):
  for col in FILE.iter_cols(5,5):
    search_term = col[y].value
    SYMBOL = print(f'Symbol of the company : {search_term}\n')
  for col in FILE.iter_cols(4,4):
    company_name = col[y].value
    COMPANY = print(f'Company Name : {company_name}')  
  for col in FILE.iter_cols(2,2):
    country_name = col[y].value
    COUNTRY = print(f'Country Name : {country_name}')
                              #   To search for stocks
  y += 3 # To iterate over stocks only
  model_dev_viz(search_term,company_name,start_date,end_date)


FILE['G2'] = end_date
READ_FILE.save('TELECOM_INPUT_FILE.xlsx')
tf.keras.backend.clear_session()

df = pd.read_csv('Stock_Data.csv')
inp = pd.read_excel('TELECOM_INPUT_FILE.xlsx')

type(df.index[-1])

inp

df.info()

df