# -*- coding: utf-8 -*-
"""Nokia_Tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GUlXuHURgTcI3nzV-6BJoHUmmKMQj6ms
"""

# !pip install pandas==1.3.5
# !pip install vaderSentiment
# !pip install snscrape

#Loading libraries
import pandas as pd
import numpy as np
from openpyxl import load_workbook
import requests
import time 
import datetime
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from numpy import array
from datetime import timedelta, datetime
import openpyxl
import snscrape
import snscrape.modules.twitter as sntwitter
from snscrape.base import ScraperException
from time import sleep
import os

tu = pd.read_excel('TELECOM_INPUT_FILE.xlsx')
tu

# FILE['G2'] = (end_time+timedelta(days=-1))

#LOAD WORKBOOK TELECOM_INPUT_FILE FOR FETCHING INPUT.
#Define variable to load the dataframe from the file.
READ_FILE = openpyxl.load_workbook("TELECOM_INPUT_FILE.xlsx")
#Define variable to read sheet.
FILE = READ_FILE.active

def get_twitter_data(search_term,start_time,end_time,company):
  tweet_data=[]
  max_results = 5000
  tweet_set = set()
  try:
    for i, tweets in enumerate(sntwitter.TwitterSearchScraper(f'{search_term} + since:{start_time} until:{end_time} -filter:links -filter:replies lang:en').get_items()):
      if i%200==0:
        print(i)
      # print('it works')
      if (tweets.user.username,tweets.rawContent) not in tweet_set:
        tweet_set.add((tweets.user.username,tweets.rawContent))
        tweet_data.append([tweets.date, tweets.rawContent,tweets.user.username])

    df = pd.DataFrame(tweet_data, columns = ['Date', 'Tweets','UserName'])# New data
    df['Company']=company# Set COmpany col
    df.set_index('Date',inplace=True)#Set index
    df.index = pd.to_datetime(df.index,errors='coerce').tz_localize(None)
    analyzer = SentimentIntensityAnalyzer()
    sentiment = []
    negative =[]
    positive=[]
    neutral=[]
    for n in range(df.shape[0]):
      Tweets = df.iloc[n,0]
      Tweets_analyzed = analyzer.polarity_scores(Tweets)
      sentiment.append(Tweets_analyzed['compound'])
      positive.append(Tweets_analyzed['pos'])
      negative.append(Tweets_analyzed['neg'])
      neutral.append(Tweets_analyzed['neu'])



    df["Sentiment"] = sentiment
    df['Negative'] = negative
    df['Positive'] = positive
    df['Neutral'] = neutral

    return df
  except ScraperException:
    df = pd.DataFrame(columns = ['Date', 'Tweets','UserName','Sentiment'])
    df['Company'] = company
    return df

#Iterate through the country column of file to print the country name.
i = 1
# We will fetch twitter data within one week
for col in FILE.iter_cols(6,6):
  start_time = col[1].value
  if isinstance(start_time,datetime):
    start_time = start_time.date()
for col in FILE.iter_cols(7,7):
  end_time = col[1].value
  if isinstance(end_time,datetime):
    end_time = end_time.date()

# start_time = end_time+timedelta(weeks=-1)
end_time=end_time+timedelta(days=1)
for col in FILE.iter_cols(2,2):
  COUNTRY = col[i].value
  COUNTRY = print(f'COUNTRY : {COUNTRY}\n')
         
  j = 1
for col in FILE.iter_cols(3,3):
  KPI = col[j].value
  KPI = print(f'KPI : {KPI}\n')
            
    # tweet_data=[]
k = 1
while(k<=15):
  for col in FILE.iter_cols(4,4):
    company=col[k].value
    COMPANY =print(f'Comapany NAME: {company}\n')
  for col in FILE.iter_cols(5,5):
    search_term = col[k].value
    OPERATOR = print(f'OPERATOR NAME: {search_term}\n')
  k += 3
  if os.path.isfile('Tweet_All.csv')==False:# if file doesnt exist just to_csv this 
    df = get_twitter_data(search_term,start_time,end_time,company)  
    df.sort_index(ascending=False,inplace=True)
    print('making file')
    df.dropna(axis=0,how='any',inplace=True)
    # df['Company'] = company
    df.to_csv('Tweet_All.csv',encoding='utf-8')
  else:# If file exists
    df_og = pd.read_csv('Tweet_All.csv',encoding='utf-8')
    df_og.set_index('Date',inplace=True)
    df_og.sort_index(inplace=True)
    if df_og[df_og.Company==company].empty:# If that comapny data doesnt exist
      df = get_twitter_data(search_term,start_time,end_time,company)
      print(f'{company} data')
      # print(df)
      # df['Company'] = company
      df_final=pd.concat([df_og,df])
      df_final.index= pd.to_datetime(df_final.index)
      df_final.sort_index(ascending=False,inplace=True)# Simple concat
      df_final.dropna(axis=0,how='any',inplace=True)
      df_final.to_csv('Tweet_All.csv',encoding='utf-8')
            
    else:
      format_data = "%Y-%m-%d %H:%M:%S"
      start_index = datetime.strptime(df_og[df_og.Company==company].index[-1],format_data).date()
      start_index = start_index+timedelta(days=1)
      print(start_index)
      df=get_twitter_data(search_term,start_index,end_time,company)
      if df.empty:
        continue
      print('date change')
      # df = df.resample('D').mean().dropna(axis=0)
      # df['Company'] = company
      # print(df)
      df_final = pd.concat([df_og,df])
      # format = '%Y-%m-%d %H:%M:%S'
      df_final.index= pd.to_datetime(df_final.index)
      df_final.sort_index(ascending=False,inplace=True)
      df_final.dropna(axis=0,how='any',inplace=True)
      df_final.to_csv('Tweet_All.csv',encoding='utf-8')

    print('--------PROCESS COMPLETED--------\n')

FILE['G2'] = (end_time)
READ_FILE.save('TELECOM_INPUT_FILE.xlsx')

#!snscrape --version

def resample_df(df):
  format = '%Y-%m-%d %H:%M:%S'
  le = df.drop(['Tweets','UserName'],axis=1)
  le.Date = pd.to_datetime(le.Date,format=format)
  re =  le.set_index(['Date']).groupby(pd.Grouper('Company')).resample('D').mean().dropna(axis=0).reset_index()
  re.to_csv('Tweet.csv',index=False)
  return re

ml = pd.read_csv('Tweet_All.csv')
ml

mn = resample_df(ml)
mn

mn = pd.read_csv('Tweet.csv')
mn

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU

def loss_plot(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(len(loss))
  plt.plot(epochs, loss, 'r', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend(loc=0)
  plt.figure()
  plt.show()

def lstm_model(x_train,y_train,shape):
  model = Sequential()
  model.add(LSTM(128, return_sequences=True, input_shape=(shape, 1)))
  model.add(Dropout(0.4))
  model.add(LSTM(128, return_sequences=False))
  # model.add(Dropout(0.4))
  model.add(Dense(64))
  model.add(Dropout(0.2))
  model.add(Dense(1))
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.008), loss='mean_squared_error')
  reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.001)

  # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,mode="min")
  history = model.fit(x_train, y_train, batch_size=1, epochs=100,validation_split=0.1,verbose=1,callbacks=[reduce_lr])
  loss_plot(history)
  return model

def split_training_data(train_data,history_range):
  #Split the data into x_train and y_train data sets
  x_train = []
  y_train = []
  n_future = 31   # Number of days we want to look into the future based on the past days.
  n_past = history_range  # Number of past days we want to use to predict the future.

#Reformat input data into a shape: (n_samples x timesteps x n_features)
#In my example, my df_for_training_scaled has a shape (12823, 5)
#12823 refers to the number of data points and 5 refers to the columns (multi-variables).
  for i in range(n_past, len(train_data) - n_future +1):
    x_train.append(train_data[i - n_past:i])
    y_train.append(train_data[i:i + 1])
  
  X,y = np.array(x_train), np.array(y_train) 
  return X,y

def plot_results(historical_data,preds,legend,search_term):
  plt.figure(figsize=(16,8))
  plt.title('Sentiment Forecast of  '+f'{search_term}')
  plt.xlabel('Date', fontsize=18)
  plt.ylabel('Sentiment', fontsize=18)
  plt.plot(historical_data)
  plt.plot(preds)
  plt.legend(legend, loc='lower right')
  plt.show()

import math
from sklearn.preprocessing import MinMaxScaler
def model_dev_viz(search_term,company_name):
  df = pd.read_csv('Tweet.csv')
  df = df[(df.Company==company_name)]
  df.Date = pd.to_datetime(df.Date).dt.date
  df = df.set_index('Date')
  data = df.filter(['Sentiment'])
  if len(df)==0:
    print('No data for this time period maybe due to weekend')
    return 
  # Stock prediction using LSTM
  print(f"DF Shape - {df.shape}")
  history_range = 5
  if df.shape[0]<history_range:
    print('No LSTM can be made for this')
    return
  # data = df.set_index('Date')
  data.index=pd.to_datetime(data.index)
  # Convert the dataframe to a numpy array
  dataset = data['Sentiment'].values
  # print(dataset)
  # print(dataset)
  # Get the number of rows to train the model on
  training_data_len = math.ceil( len(dataset) * 0.8 )
  # Create the training data set 
  train_data = dataset[:training_data_len]
  # print(train_data)
  
  # Convert the x_train and y_train to numpy arrays
  x_train, y_train = split_training_data(train_data,history_range)

  # Reshape the data
  # x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
  print(f"Train X shape -  {x_train.shape}")

  # Build the LSTM model
  model = lstm_model(x_train,y_train,x_train.shape[1])
  # GRU Model
  # model = opt_gru_model(x_train,y_train,x_train.shape[1])
  # print(model.summary())
  # Create the testing data set
  # test_model(scaled_data,dataset,training_data_len)
  test_data = dataset[training_data_len - history_range:]
  # print('Test Data - ')
  # print(test_data)
  # Create the data set x_test and y_test
  x_test = []
  y_test = dataset[training_data_len:]
  for i in range(history_range, len(test_data)):
    x_test.append(test_data[i-history_range:i])
  # Convert the data to a numpy array
  x_test = np.array(x_test)

  # Reshape the data
  x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

  # Get the model's predicted price values for the x_test data set
  predictions = model.predict(x_test)
  # predictions = scaler.inverse_transform(predictions)
  # print(f'Predictions - \n {predictions}')

  # Evaluate model (get the root mean quared error (RMSE))
  rmse = np.sqrt( np.mean( predictions - y_test )**2 )
  print(f"rmse error on test dataset- {rmse}")
  
  train = data[:training_data_len]
  valid = data[training_data_len:]
  # print(train)
  valid['Predictions'] = predictions
  # Visualize the data
  plot_results(train['Sentiment'],valid[['Sentiment', 'Predictions']],['Train', 'Val', 'Predictions'],search_term)
  # Future prediction data
  X_FUTURE = 31
  predictions = np.array([])
  last = x_test[-1]
  for i in range(X_FUTURE):
    curr_prediction = model.predict(np.array([last]))
    last = np.concatenate([last[1:], curr_prediction])
    predictions = np.concatenate([predictions, curr_prediction[0]])
  # predictions = scaler.inverse_transform([predictions])[0]

  dicts = []
  # print(data)
  curr_date = data.index[-1]
  print(curr_date)
  for i in range(X_FUTURE):
    curr_date = curr_date + timedelta(days=1)
    print(curr_date)
    dicts.append({'Predicted_Sentiment':predictions[i], "Future_Date": curr_date})

  new_data = pd.DataFrame(dicts)
  new_data['Company'] = company_name
  # new_data.reset_index
  if os.path.isfile('Tweet1.csv')==False:
    new_data.to_csv('Tweet1.csv',index=False)
  else:
    pr = pd.read_csv('Tweet1.csv')
    new_pr = pd.concat([pr,new_data])
    new_pr.to_csv('Tweet1.csv',index=False)
  # append_predictions(new_data,company_name)
  # Plot the data
  train = data
  # Visualize the data
  new_data.set_index('Future_Date',inplace=True)
  plot_results(train['Sentiment'],new_data["Predicted_Sentiment"],['Train', 'Predictions'],search_term)
  
  print('--------PROCESS COMPLETED--------\n')

i = 1
# We will fetch twitter data within one week
for col in FILE.iter_cols(6,6):
  start_time = col[1].value
  if isinstance(start_time,datetime):
    start_time = start_time.date()
for col in FILE.iter_cols(7,7):
  end_time = col[1].value
  if isinstance(end_time,datetime):
    end_time = end_time.date()

# start_time = end_time+timedelta(weeks=-1)
end_time=end_time+timedelta(days=1)
for col in FILE.iter_cols(2,2):
  COUNTRY = col[i].value
  COUNTRY = print(f'COUNTRY : {COUNTRY}\n')
         
  j = 1
for col in FILE.iter_cols(3,3):
  KPI = col[j].value
  KPI = print(f'KPI : {KPI}\n')
            
    # tweet_data=[]
k = 1
while(k<=15):
  for col in FILE.iter_cols(4,4):
    company=col[k].value
    COMPANY =print(f'Comapany NAME: {company}\n')
  for col in FILE.iter_cols(5,5):
    search_term = col[k].value
    OPERATOR = print(f'OPERATOR NAME: {search_term}\n')
  k += 3
  model_dev_viz(search_term,company)